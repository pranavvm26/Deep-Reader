{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting editdistance==0.5.3\n",
      "  Downloading editdistance-0.5.3-cp37-cp37m-manylinux1_x86_64.whl (179 kB)\n",
      "\u001b[K     |████████████████████████████████| 179 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: editdistance\n",
      "Successfully installed editdistance-0.5.3\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting numba==0.49.0\n",
      "  Downloading numba-0.49.0-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from numba==0.49.0) (46.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/site-packages (from numba==0.49.0) (1.18.4)\n",
      "Collecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n",
      "  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 37.3 MB/s eta 0:00:01    |██████████████▎                 | 9.0 MB 37.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.32.1 numba-0.49.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install editdistance==0.5.3\n",
    "!pip3 install numba==0.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "from data.generator import DataGenerator, Tokenizer\n",
    "from network.model import HTRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (64, 32, 1)\n",
    "max_text_length = 32\n",
    "charset_base = string.printable[:95]\n",
    "\n",
    "source='iam_cvl'\n",
    "arch =\"puigcerver\" # puigcerver, bluche, flor,\n",
    "batch_size = 350\n",
    "lr = 0.0001\n",
    "type_of_run = 'train'\n",
    "if type_of_run == 'train':\n",
    "    train_model = False\n",
    "else:\n",
    "    train_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = os.path.join(\"/floyd/input/words_htr_combo/{0}.hdf5\".format(source))\n",
    "output_path = os.path.join(\"/floyd/home/output_words_combo\", source, arch)\n",
    "target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n",
    "\n",
    "assert os.path.isfile(source_path) or os.path.isfile(target_path)\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/floyd/input/words_htr_combo/iam_cvl.hdf5\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 64, 32, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 64, 32, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 64, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 32, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 32, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 16, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 16, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 16, 8, 64)         18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 16, 8, 64)         256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 16, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 16, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 16, 8, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 16, 8, 128)        512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 16, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 16, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 16, 8, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 16, 8, 128)        512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 16, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 16, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 16, 8, 80)         92240     \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 16, 8, 80)         320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 16, 8, 80)         0         \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 32, 320)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio (None, 32, 512)           1181696   \n",
      "_________________________________________________________________\n",
      "bidirectional_37 (Bidirectio (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_38 (Bidirectio (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_39 (Bidirectio (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_40 (Bidirectio (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32, 98)            50274     \n",
      "=================================================================\n",
      "Total params: 11,020,210\n",
      "Trainable params: 11,019,314\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dtgen = DataGenerator(source=source_path,\n",
    "                      batch_size=batch_size,\n",
    "                      charset=charset_base,\n",
    "                      max_text_length=max_text_length,\n",
    "                      predict=train_model)\n",
    "\n",
    "model = HTRModel(architecture=arch,\n",
    "                 input_size=input_size,\n",
    "                 vocab_size=dtgen.tokenizer.vocab_size,\n",
    "                 beam_width=10,\n",
    "                 stop_tolerance=20,\n",
    "                 reduce_tolerance=15)\n",
    "\n",
    "model.compile(learning_rate=lr)\n",
    "model.load_checkpoint(target=target_path)\n",
    "\n",
    "model.summary(output_path, \"summary.txt\")\n",
    "callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 121562\n",
      "Valid images: 6421\n",
      "Test images: 14209\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train images: {dtgen.size['train']}\")\n",
    "print(f\"Valid images: {dtgen.size['valid']}\")\n",
    "print(f\"Test images: {dtgen.size['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "348/348 [==============================] - ETA: 0s - loss: 17.6067\n",
      "Epoch 00001: val_loss improved from inf to 17.62555, saving model to /floyd/home/output_words_combo/iam_cvl/puigcerver/checkpoint_weights.hdf5\n",
      "348/348 [==============================] - 298s 857ms/step - loss: 17.6067 - val_loss: 17.6255 - lr: 1.0000e-04\n",
      "Epoch 2/400\n",
      "268/348 [======================>.......] - ETA: 1:05 - loss: 17.5519"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "h = model.fit(x=dtgen.next_train_batch(),\n",
    "              epochs=400,\n",
    "              steps_per_epoch=dtgen.steps['train'],\n",
    "              validation_data=dtgen.next_valid_batch(),\n",
    "              validation_steps=dtgen.steps['valid'],\n",
    "              callbacks=callbacks,\n",
    "              shuffle=True,\n",
    "              verbose=1)\n",
    "\n",
    "total_time = datetime.datetime.now() - start_time\n",
    "\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "\n",
    "min_val_loss = min(val_loss)\n",
    "min_val_loss_i = val_loss.index(min_val_loss)\n",
    "\n",
    "time_epoch = (total_time / len(loss))\n",
    "total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
    "\n",
    "t_corpus = \"\\n\".join([\n",
    "    f\"Total train images:      {dtgen.size['train']}\",\n",
    "    f\"Total validation images: {dtgen.size['valid']}\",\n",
    "    f\"Batch:                   {dtgen.batch_size}\\n\",\n",
    "    f\"Total time:              {total_time}\",\n",
    "    f\"Time per epoch:          {time_epoch}\",\n",
    "    f\"Time per item:           {time_epoch / total_item}\\n\",\n",
    "    f\"Total epochs:            {len(loss)}\",\n",
    "    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
    "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
    "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
    "])\n",
    "\n",
    "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
    "    lg.write(t_corpus)\n",
    "    print(t_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
